(p <- ggplot(av, aes(x = trial, y = RTs)) +
geom_point() + facet_wrap(~ participant))
# fit a simple lm
m_av <- lm(RTs ~ trial, data = av)
p + geom_smooth(method = "lm", se = F)
p + geom_abline(aes(intercept = coef(m_av)[1],
slope = coef(m_av)[2]),
lty = 2, col = "blue")
# fit a model with varying intercept
m_av01 <- lmer(RTs ~ trial + (1 | participant), data = av)
# get coefficients for individual participants:
m_av01_coef <- coef(m_av01)$participant %>% rownames_to_column() %>%
setNames(c("participant", "intercept", "slope"))
# visualize varying intercepts:
left_join(av, m_av01_coef) %>%
ggplot(aes(x = trial, y = RTs)) +
geom_point() + facet_wrap(~ participant) +
geom_abline(aes(intercept = intercept, slope = slope),
col = "blue", lty = 2)
# add varying slopes:
m_av02 <- lmer(RTs ~ trial + (1 + trial | participant), data = av)
# get coefficients:
m_av02_coef <- coef(m_av02)$participant %>% rownames_to_column() %>%
setNames(c("participant", "intercept02", "slope02"))
left_join(left_join(av, m_av01_coef), m_av02_coef) %>%
ggplot(aes(x = trial, y = RTs)) +
geom_point() + facet_wrap(~ participant) +
geom_abline(aes(intercept = intercept, slope = slope),
col = "blue", lty = 2) +
geom_abline(aes(intercept = intercept02, slope = slope02),
col = "red", lty = 3) +
geom_abline(aes(intercept = coef(m_av)[1],
slope = coef(m_av)[2]),
lty = 4, col = "grey")
# load data (from languageR)
data("lexdec")
# remove outliers, only use correct answers
lexdec2 <- lexdec[lexdec$RT < 7, ]
lexdec3 <- lexdec2[lexdec2$Correct == "correct", ]
# center relevant data
lexdec3$Frequency.c <- scale(lexdec3$Frequency, scale = F)
lexdec3$Trial.c <- scale(lexdec3$Trial, scale = F)
# plot each participant in a single panel
xylowess.fnc(RT ~ Trial | Subject, data = lexdec3,
ylabel = "log RT", xlabel = "Trial")
# model: Reaction Time ~ Frequency
m1 <- lme4::lmer(RT ~ Frequency.c +
(1 | Subject) +
(1 | Word),
data = lexdec3,
REML = F) # REML: restricted maximum likelihood
factorial(20)
set.seed(1)
n <- 100
tr <- rbinom(100, 1, 0.5)
y <- 1 + tr + rnorm(n, 0, 3)
tr
y
diff(by(y, tr, mean))
y <- c(6,5,5,3,4,5,6,7)
diff(by(y, tr, mean))
y <- 1 + tr + rnorm(n, 0, 3)
y
str(y)
y <- c(6,5,5,3,4,5,6,7)
str(y)
tr
y <- sample(1:6, 100)
y <- sample(1:6, 100, replace = T)
diff(by(y, tr, mean))
# install a package
!require("tidyerse")
# install a package
!require("tidyverse")
69+28.95
47.5+20.75
library(collostructions)
?collex.distst
?collex.dist
set.seed(123)
library(zipfR)
set.seed(123)
d <- data.frame(cxn = sample(c("A", "B"), 100, replace = T),
freq1 = rnorm(100, mean = 50, sd = 20),
freq2 = rnorm(100, mean = 50, sd = 20))
d
set.seed(123)
d <- data.frame(cxn = sample(c("A", "B"), 100, replace = T),
freq1 = round(rnorm(100, mean = 50, sd = 20)),
freq2 = round(rnorm(100, mean = 50, sd = 20)))
d
collex.dist(d)
subset(collex.dist(d), SIGNIF != "ns")
d2 <- d
d2$freq1 <- d2$freq1*10
d2$freq2 <- d2$freq2*10
subset(collex.dist(d), SIGNIF != "ns")
head(subset(collex.dist(d), SIGNIF != "ns"))
head(subset(collex.dist(d), SIGNIF != "ns"))
head(subset(collex.dist(d2), SIGNIF != "ns"))
head(subset(collex.dist(d), SIGNIF != "ns"))
head(subset(collex.dist(d2), SIGNIF != "ns"))
fion <- readxl::read_xlsx("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/fion_wordlist.xlsx")
View(fion)
silvie <- readxl::read_xlsx("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist02.xlsx")
View(silvie)
sum(fion$n)
sum(fion$n_CHI)
sum(fion$n_CDS)
sum(silvie$n_CHI)
sum(silvie$n_CDS)
sum(fion$n_CHI)+sum(fion$n_CDS)+sum(silvie$n_CHI)+sum(silvie$n_CDS)
library(tidyverse)
library(readxl)
?readxl::excel_sheets
excel_sheets("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/fion_wordlist.xlsx")
length(excel_sheets("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/fion_wordlist.xlsx"))
fion$n_CHI+fion$n_CDS
sum(fion$n_CHI)+sum(fion$n_CDS)
for(i in 1:21) {
fion <- read_xlsx("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/fion_wordlist.xlsx", sheet = i)
if(i == 1) {
n_all <- sum(fion$n_CHI)+sum(fion$n_CDS)
} else {
n_all <- n_all + sum(fion$n_CHI)+sum(fion$n_CDS)
}
}
length(excel_sheets("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist.xlsx"))
length(excel_sheets("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist02.xlsx"))
for(i in 1:10) {
silvie <- read_xlsx("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist02.xlsx", sheet = i)
if(i == 1) {
n_all <- sum(silvie$n_CHI)+sum(silvie$n_CDS)
} else {
n_all <- n_all + sum(silvie$n_CHI)+sum(silvie$n_CDS)
}
}
library(tidyverse)
library(readxl)
length(excel_sheets("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/fion_wordlist.xlsx"))
length(excel_sheets("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist02.xlsx"))
for(i in 1:21) {
fion <- read_xlsx("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/fion_wordlist.xlsx", sheet = i)
if(i == 1) {
n_all <- sum(fion$n_CHI)+sum(fion$n_CDS)
} else {
n_all <- n_all + sum(fion$n_CHI)+sum(fion$n_CDS)
}
}
for(i in 1:10) {
silvie <- read_xlsx("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist02.xlsx", sheet = i)
if(i == 1) {
n_all_silvie <- sum(silvie$n_CHI)+sum(silvie$n_CDS)
} else {
n_all_silvie <- n_all + sum(silvie$n_CHI)+sum(silvie$n_CDS)
}
}
n_all+n_all_silvie
f <- list.files("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist", full.names = T)
f <- list.files("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist", full.names = T)
for(i in 1:length(f)) {
silvie <- read_xlsx(f[i])
if(i == 1) {
n_all_silvie <- sum(silvie$n_CHI)+sum(silvie$n_CDS)
} else {
n_all_silvie <- n_all + sum(silvie$n_CHI)+sum(silvie$n_CDS)
}
}
n_all+n_all_silvie
f <- list.files("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/lily_wordlist", full.names = T)
for(i in 1:length(f)) {
lily <- read_xlsx(f[i])
if(i == 1) {
n_all_lily <- sum(lily$n_CHI)+sum(lily$n_CDS)
} else {
n_all_lily <- n_all + sum(lily$n_CHI)+sum(lily$n_CDS)
}
}
n_all+n_all_silvie+n_all_lily
install.packages("quanteda")
library(gutenbergr)
gutenberg_languages
filter(gutenberg_languages, language=="de")
library(tidyverse)
gl <- gutenberg_languages
filter(gl, language=="de")
glde <- filter(gl, language=="de")
gutenberg_metadata
gutenberg_metadata
586.2*6
586.2*9
586.2*6
586.2*8
707.62*6
4689.6+4245.72
544.33*4
544.33*5
544.33*6
544.33*4
544.33*5
3000/707.62
707.62*4
707.62*4.5
10+6+2+8+18
# relative Bestehensgrenze:
117.5*0.78
# Punktverteilung
seq(91, 160, length.out = 11)
127+139+148+145+119+112
790/6
0.78*131.66
60+88+69
217/3
library(tidyverse)
d <- read_csv("/Users/stefanhartmann/sciebo/ARENAS WP2/Data DE 12022024/germany.csv")
d$Text
grep("#genderwahn", d)
grep("#genderwahn", d$Text)
grep("genderwahn", d$Text, ignore.case = T)
grep("gender", d$Text, ignore.case = T)
library(writexl)
d[grep("gender", d$Text, ignore.case = T),]
d[grep("gender", d$Text, ignore.case = T),] %>% write_xlsx("gender_twitter.xlsx")
d <- read_csv("/Users/stefanhartmann/sciebo/ARENAS WP2/Data DE 12022024/facebook-de.csv")
d[grep("gender", d$`Post, Text`, ignore.case = T),] %>% write_xlsx("gender_twitter.xlsx")
d <- read_csv("/Users/stefanhartmann/sciebo/ARENAS WP2/Data DE 12022024/germany.csv")
d[grep("gender", d$Text, ignore.case = T),] %>% write_xlsx("gender_twitter.xlsx")
d <- read_csv("/Users/stefanhartmann/sciebo/ARENAS WP2/Data DE 12022024/facebook-de.csv")
d[grep("gender", d$Text, ignore.case = T),] %>% write_xlsx("gender_fb.xlsx")
d[grep("gender", d$`Post, Text`, ignore.case = T),] %>% write_xlsx("gender_fb.xlsx")
d[grep("gender", d$`Post, Text`, ignore.case = T),]
d <- read_csv("/Users/stefanhartmann/sciebo/ARENAS WP2/Data DE 12022024/youtube-GER-clean.csv")
d[grep("gender", d$`Post, Text`, ignore.case = T),] %>% write_xlsx("gender_youtube.xlsx")
d[grep("gender", d$`Comment, Text`, ignore.case = T),] %>% write_xlsx("gender_youtube.xlsx")
d[grep("gender", d$`Comment, Text`, ignore.case = T),]
setwd("~/sciebo/Projekte/werdenFutur/EN_going_to_will/analysis_OANC_BNC_NOR_Feb2024")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Chunk 2
library(tidyverse)
library(readxl)
library(party)
library(lattice)
library(Hmisc)
library(pdp)
library(collostructions) # available at sfla.ch
library(patchwork)
library(ggparty)
library(Boruta)
library(svglite)
# Chunk 4
# read data: Norwegian
nor_bb <- read_xlsx("data/Norwegian/nota_bb_vil_skal_komer_til_a.xlsx", sheet = "bigbrother")
nor_nota <- read_xlsx("data/Norwegian/nota_bb_vil_skal_komer_til_a.xlsx", sheet = "nota")
nor <- rbind(nor_nota, nor_bb)
# clean column names
colnames(nor) <- gsub(" ", "_", colnames(nor))
# read data: English
oanc <- read_csv("data/English/OANC/oanc_spoken_going_to_shall_will_5000.csv")
bnc  <- read_xlsx("data/English/SpokenBNC2014/SPOKENBNC2014_spoken_will_shall_going_to_sample_5000-final-priming.xlsx")
# Chunk 5
# add protasis/apodosis annotation to BNC
bnc$if_clause2 <- ifelse(bnc$if_clause=="if" & bnc$subordinate=="sub", "protasis", NA)
bnc$if_clause2 <- ifelse(bnc$if_clause=="if" & bnc$subordinate=="main", "apodosis", bnc$if_clause2)
bnc$if_clause2 <- ifelse(bnc$if_clause!="if", "no", bnc$if_clause2)
# add protasis/apodosis annotation to OANC
oanc$if_clause2 <- ifelse(oanc$if_clause=="if" & oanc$subordinate=="sub", "protasis", NA)
oanc$if_clause2 <- ifelse(oanc$if_clause=="if" & oanc$subordinate=="main", "apodosis", oanc$if_clause2)
oanc$if_clause2 <- ifelse(oanc$if_clause!="if", "no", oanc$if_clause2)
# add protasis/apodosis annotation to NOR
nor$if_clause2 <- ifelse(nor$`If-clause`=="if" & nor$Clause_type=="sub", "protasis", "NA")
nor$if_clause2 <- ifelse(nor$`If-clause`=="if" & nor$Clause_type=="main", "apodosis", nor$if_clause2)
nor$if_clause2 <- ifelse(nor$`If-clause`!="if", "no", nor$if_clause2)
# Chunk 6
# OANC ---------
# binary:
oanc$cxn <- ifelse(oanc$Key %in% c("gonna", "going"), "going_to", "will")
# more fine-grained:
oanc$cxn1 <- case_when(oanc$Key == "will" ~ "will",
oanc$Key == "'ll" ~ "'ll",
oanc$Key == "shall" ~ "shall",
oanc$Key == "wo" ~ "won't",
oanc$Key == "going" ~ "going to",
oanc$Key == "gonna" ~ "gonna")
oanc$cxn1 <- factor(oanc$cxn1, levels = c("will", "'ll", "shall", "won't", "going to", "gonna"))
# BNC ---------
# binary
bnc$cxn <- ifelse(bnc$Key %in% c("'ll", "will", "wo"), "will", "going to")
bnc$cxn1 <- case_when(bnc$Key == "gon" ~ "gonna",
bnc$Key == "'ll" ~ "'ll",
bnc$Key == "going" ~ "going to",
bnc$Key == "will" ~ "will",
bnc$Key == "wo" ~ "won't",
bnc$Key == "shall" ~ "shall")
bnc$cxn1 <- factor(bnc$cxn1, levels = c("will", "'ll", "shall", "won't", "going to", "gonna"))
# Chunk 7
bnc  <- bnc %>% filter(keep == "y")
oanc <- oanc %>% filter(keep=="y")
# Chunk 8
left_join(setNames(as.data.frame(table(bnc$cxn1)), c("Cxn", "BNC")),
setNames(as.data.frame(table(oanc$cxn1)), c("Cxn", "OANC")))
# Chunk 9
# Norwegian: ---------------
# relevant variables as factors
nor$Cx <- factor(nor$Cx)
nor$Negative <- factor(nor$Negative)
nor$Interrogative <- factor(nor$Interrogative)
nor$if_clause2 <- factor(nor$if_clause2)
nor$Clause_type <- factor(nor$Clause_type)
# reduce number of levels for Lexeme
nor$Lexeme <- factor(nor$Lexeme)
nor$lexeme <- fct_lump_min(nor$Lexeme, min = 50, other_level = "other")
# more descriptive name
nor$Construction <- nor$Cx
# shorter cx labels (obsolete now that we're using ggparty)
nor$cxn <- case_when(nor$Cx == "kommer" ~ "k",
nor$Cx == "skal" ~ "s",
nor$Cx == "vil" ~ "v")
nor$cxn <- factor(nor$cxn)
# shorter if clause label
nor$if_clause <- nor$if_clause2
# CART tree
set.seed(19851003)
tr_nor <- ctree(Construction ~ Negative+Interrogative+if_clause+Clause_type,
data = nor)
# plot - adapted from https://ladal.edu.au/tree.html
# extract p-values
pvals <- unlist(nodeapply(tr_nor, ids = nodeids(tr_nor), function(n) info_node(n)$p.value))
pvals <- pvals[pvals <.05]
( tr_nor_plot <- ggparty(tr_nor) +
geom_edge() +
geom_edge_label() +
geom_node_label(line_list = list(aes(label = splitvar),
aes(label = paste0("N=", nodesize, ", p",
ifelse(pvals < .001, "<.001", paste0("=", round(pvals, 3)))),
size = 10)),
line_gpar = list(list(size = 13),
list(size = 10)),
ids = "inner") +
geom_node_label(aes(label = paste0("Node ", id, ", N = ", nodesize)),
ids = "terminal", nudge_y = -0.0, nudge_x = 0.01) +
geom_node_plot(gglist = list(
geom_bar(aes(x = "", fill = Construction),
position = position_fill(), color = "black"),
theme_minimal(),
scale_fill_grey(start = .4, end = .9),
scale_y_continuous(breaks = c(0, 1)),
xlab(""),
ylab("Probability"),
geom_text(aes(x = "", group = Construction,
label = after_stat(count)),
stat = "count", position = position_fill(), vjust = 1.1)),
shared_axis_labels = TRUE) )
# export as pdf (ggsave not working for ggparty objects apparently)
# png("figures/tree_NO.png", width = 10, height = 6, un = "in", res = 300)
# plot(tr_nor)
# dev.off()
# Chunk 12
# re-import random forest
for_nor <- read_rds("rds/for_nor.rds")
# re-import variable importance
vi      <- read_rds("rds/vi.rds")
# Chunk 13
(vi_plot_nor <- as.data.frame(vi) %>% rownames_to_column() %>% ggplot(aes(y = fct_reorder(rowname, vi), x = vi)) +
geom_point(size = 5) + xlim(min(vi), max(vi)) +
theme_bw() +
theme(panel.grid.major.x = element_blank()) +
theme(panel.grid.minor = element_blank()) + xlab("Conditional variable importance") + ylab("Variable") +
theme(axis.text = element_text(size = 18)) +
theme(axis.title = element_text(size = 18)) +
theme(strip.text = element_text(size = 18)) +
theme(legend.text = element_text(size = 18)) +
theme(legend.title = element_text(size = 18, face = "bold")) +
theme(text = element_text(size = 18)))
# Chunk 16
# re-import fitted values
pred.nor <- read_rds("rds/pred_nor.rds")
# Chunk 17
# proportion of correct predictions
sum(as.numeric(sapply(1:length(pred.nor), function(i) pred.nor[i] == nor$Cx[i]))) / length(pred.nor)
# Chunk 18
# combine OANC and BNC2014 data
colnames(oanc)[which(colnames(oanc)=="gramm_person")] <- "Gramm_person"
colnames(oanc)[which(colnames(oanc)=="animacy")] <- "Animacy"
colnames(oanc)[which(colnames(oanc)=="question")] <- "Interrogative"
colnames(bnc)[which(colnames(bnc)=="question")] <- "Interrogative"
colnames(bnc)[which(colnames(bnc)=="subordinate")] <- "Clause_type"
colnames(oanc)[which(colnames(oanc)=="subordinate")] <- "Clause_type"
colnames(oanc)[which(colnames(oanc)=="negation")] <- "Negative"
colnames(bnc)[which(colnames(bnc)=="negation")] <- "Negative"
eng <- rbind(
mutate(select(oanc, Left, Key, Right, Clause_type, Gramm_person, Animacy, Interrogative, Negative, if_clause, if_clause2, cxn, cxn1, Lemma), corpus = "OANC-Spoken"),
mutate(select(bnc, Left, Key, Right,  Clause_type, Gramm_person, Animacy, Interrogative, Negative, if_clause, if_clause2, cxn, cxn1, Lemma), corpus = "SpokenBNC2014")
)
# CART tree
eng$Negative <- factor(eng$Negative)
eng$Interrogative <- factor(eng$Interrogative)
eng$if_clause2 <- factor(eng$if_clause2)
eng$Clause_type <- factor(eng$Clause_type)
eng$cxn <- factor(eng$cxn)
eng$cxn1 <- factor(eng$cxn1)
# omit NAs
eng1 <- select(eng, Negative, Interrogative, if_clause2, Clause_type, cxn1, corpus, Lemma)
eng1 <- na.omit(eng1)
# abbreviate labels - obsolete using ggparty
eng1$cxn <- case_when(eng1$cxn1=="will" ~ "w",
eng1$cxn1=="shall" ~ "s",
eng1$cxn1=="won't" ~ "wnt",
eng1$cxn1=="going to" ~ "gt",
eng1$cxn1=="gonna" ~ "gna",
eng1$cxn1=="'ll" ~ "ll")
# add lexeme
eng1$Lexeme <- factor(eng1$Lemma)
# add lexeme with reduced number of levels (bin infrequent lemmas to "other")
eng1$lexeme <- fct_lump_min(eng1$Lexeme, min = 35)
# factor
eng1$Construction <- eng1$cxn1
eng1$cxn <- factor(eng1$cxn, levels = c("w", "ll", "wnt", "s", "gt", "gna"))
# rename if-clause column to increase readability of tree diagram
eng1$if_clause <- eng1$if_clause2
# corpus as factor
eng1$corpus <- factor(eng1$corpus)
# more descriptive: use language variety instead of corpus
eng1$Variety <- ifelse(eng1$corpus == "OANC-Spoken", "AmE", "BrE")
eng1$Variety <- factor(eng1$Variety)
set.seed(1985)
tree_en <- ctree(Construction ~ Negative+Interrogative+if_clause+Clause_type+Variety,
data = eng1)
# plot
# extract p-values
pvals <- unlist(nodeapply(tree_en, ids = nodeids(tree_en), function(n) info_node(n)$p.value))
pvals <- pvals[pvals <.05]
# plot:
( tr_en_plot <- ggparty(tree_en) +
geom_edge() +
geom_edge_label() +
geom_node_label(line_list = list(aes(label = splitvar),
aes(label = paste0("N=", nodesize, ", p",
ifelse(pvals < .001, "<.001", paste0("=", round(pvals, 3)))),
size = 10)),
line_gpar = list(list(size = 13),
list(size = 10)),
ids = "inner") +
geom_node_label(aes(label = paste0("Node ", id, ", N = ", nodesize)),
ids = "terminal", nudge_y = -0.0, nudge_x = 0.01) +
geom_node_plot(gglist = list(
geom_bar(aes(x = "", fill = Construction),
position = position_fill(), color = "black"),
theme_minimal(),
scale_fill_grey(start = .4, end = .9),
scale_y_continuous(breaks = c(0, 1)),
xlab(""),
ylab("Probability"),
geom_text(aes(x = "", group = Construction,
label = after_stat(count)),
stat = "count", position = position_fill(), vjust = 1.1)),
shared_axis_labels = TRUE) )
# save as pdf via export pane
#dev.off()
# png("tree_en.png", width = 30, height = 10, un = "in", res = 300)
# plot(tree_en)
# dev.off()
# random forest
# Chunk 21
# re-import saved random forest
for_en <- read_rds("rds/for_en.rds")
# re-import
vi_en <- read_rds("rds/vi_en.rds")
# Chunk 22
(vi_plot_en <- as.data.frame(vi_en) %>% rownames_to_column() %>% ggplot(aes(y = fct_reorder(rowname, vi_en), x = vi_en)) +
geom_point(size = 5) + xlim(min(vi_en), max(vi_en)) +
theme_bw() +
theme(panel.grid.major.x = element_blank()) +
theme(panel.grid.minor = element_blank()) + xlab("Conditional variable importance") + ylab("Variable") +
theme(axis.text = element_text(size = 18)) +
theme(axis.title = element_text(size = 18)) +
theme(strip.text = element_text(size = 18)) +
theme(legend.text = element_text(size = 18)) +
theme(legend.title = element_text(size = 18, face = "bold")) +
theme(text = element_text(size = 18)))
# both plots
vi_plot_nor + ggtitle("Norwegian") +
theme(plot.title = element_text(face = "bold", hjust = 0.5)) +
plot_spacer() +
vi_plot_en + ggtitle("English") +
theme(plot.title = element_text(face = "bold", hjust = 0.5)) +
plot_layout(widths = c(4,.5,4))
# ggsave("figures/forest_NOR_EN.png", width = 12, height = 5)
# fitted values for out of bag sample
pred.eng <- predict(for_en, OOB = T)
# export fitted values
write_rds(pred.eng, "rds/pred_eng.rds")
# re-import fitted values
pred.eng <- read_rds("rds/pred_eng.rds")
table(pred.eng)
table(eng$cxn1)
table(pred.eng, eng1$cxn1)
sum(as.numeric(sapply(1:length(pred.eng), function(i) pred.eng[i] == eng1$cxn1[i]))) / length(pred.eng)
# first Boruta model for Norwegian
nor$Lexeme <- factor(nor$Lexeme)
set.seed(19551105)
boruta01 <- Boruta(Cx ~ Negative+Interrogative+if_clause+Clause_type+Lexeme,
data = nor)
# decision
getConfirmedFormula(boruta01)
plotImpHistory(boruta01)
plot(boruta01)
# Boruta model with confirmed formula
set.seed(19551105)
boruta02 <- Boruta(Cx ~ Lexeme + Clause_type + if_clause +
Interrogative + Negative,
data = nor)
